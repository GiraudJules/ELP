# Ensemble Learning Project (ELP) - MSc AI - CS 

## Credits
This project was created by Amandine Allmang, Nicolas Bourriez, Marie Bouvard, Jules Giraud, Arthur Nardone as a part of the Ensemble Learning course of the MSc AI at CentraleSupelec.

## *Project 1: AirBnB Price Prediction*
The goal of this project was to predict AirBnB prices in New York using ensemble learning methods. The code is written in Python and uses popular machine learning libraries such as Scikit-learn and XGBoost. 

### Dataset
The dataset used in this project is the [Airbnb New York City Airbnb Open Data dataset](https://www.kaggle.com/datasets/dgomonov/new-york-city-airbnb-open-data), which contains detailed information on Airbnb listings in New York City, including price, location, room type, and availability.

### Installation
To run this code, you need to have Python 3 and the following Python libraries installed:
-pandas
-numpy
-matplotlib
-plotly.graph_objects
-seaborn
-scikit-learn
-xgboost

To install these libraries, you can use pip by running:

`pip install pandas numpy matplotlib seaborn scikit-learn xgboost plotly`

### Usage
You can run the code by opening the Jupyter Notebook `AirBnB_boosting_models.ipynb` and `AirBnB_bagging_models.ipynb` and executing the cells. The notebooks contain the training of the various models and visualizations of the results. To do so, the notebooks use the different helper methods present in the Python (.py) files.

The code consists of the following steps:

- Data Exploration and Analysis: In this step, we explored the data and looked at the distribution of samples for each feature as well as the interaction between features. The data explortion notebooks can be found in the `Data_Visualization` file.
- Data Cleaning and Preprocessing: In this step, we load the dataset, handle missing values, and perform some feature engineering to prepare the data for modeling.
- Model Training: In this step, we train and compare the performance of different ensemble learning methods, including Decision Trees, Random Forest, Extremely Randomized Trees, Gradient Boosting, XGBoost and Adaboost.

## *Project 2: Decision Tree implementation*

TO COMPLETE
